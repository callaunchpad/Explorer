import gym
import time
import cv2
import scipy
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import copy

#integer for graph
u = 0

class PolicyGradient:

    def __init__(self):
        #self.data = []
        self.name = "policy"

    def policy_gradient(self, initializer, num_episodes):

        def discount(array, rate):
            sum = 0
            result = []

            for i in range(0, len(array)):
                sum = sum * rate + array[len(array) - i - 1]
                result.insert(0, sum)

            # NN like rewards between -1 and 1 like their weights
            # normalize

            result -= np.mean(result)
            result /= np.std(result)
            return result

        num_inputs = 4
        num_outputs = 2

        state_pl = tf.placeholder(tf.float32, shape=(None, num_inputs))
        reward_pl = tf.placeholder(tf.float32, shape=[None, ])
        action_pl = tf.placeholder(tf.int32, shape=[None, ])
        hidden1 = tf.layers.dense(state_pl, 30, kernel_initializer=initializer, activation=tf.nn.leaky_relu)
        hidden2 = tf.layers.dense(hidden1, 30, kernel_initializer=initializer, activation=tf.nn.leaky_relu)

        '''right or left'''
        output = tf.layers.dense(hidden2, num_outputs)
        p_output = tf.nn.softmax(output)


        with tf.variable_scope("training"):
            one_hot = tf.one_hot(action_pl, num_outputs)
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot, logits=output)
            loss = cross_entropy * reward_pl
            loss = tf.reduce_mean(loss)
            optimizer = tf.train.AdamOptimizer(0.01) #0.01
            training_Op = optimizer.minimize(loss)

        render = False

        '''OpenAI environments'''
        env = gym.make('CartPole-v0')  #'Cartpole'
        env._max_episode_steps = 1000
        state = env.reset()
        total_reward = 0
        data = []

        with tf.Session() as session:
            tf.global_variables_initializer().run()
            states = []
            actions = []
            rewards = []
            episode = 0

            while True:
                if render:
                    env.render()
                action = session.run(p_output, feed_dict={state_pl: state[np.newaxis, :]})[0]
                action = np.random.choice(np.arange(num_outputs), p=action)

                #epsilon = 0.5
                #random_num = np.random.random(1)
                #print(action)

                #if action[0] > action[1]:
                #    action = 0
                #else:
                #    action = 1

                actions.append(action)
                states.append(state)
                new_state, reward, done, info = env.step(action)

                rewards.append(reward)

                total_reward += reward
                #new_state = new_state[np.newaxis, :]
                #session.run(training_Op, feed_dict={state_pl:state, reward_pl:[reward], action_pl:[action]})
                state = new_state
                #env.render()
                if episode > num_episodes:
                    return data
                if done:
                    #print(episode, total_reward)
                    #self.data.append((episode, total_reward))
                    data.append((episode, total_reward))

                    rewards = discount(rewards, 0.99)
                    session.run(training_Op, feed_dict={state_pl: states, reward_pl: rewards, action_pl: actions})

                    # if total_reward > 600:
                    #     render = True
                    episode+=1
                    state = env.reset()
                    total_reward = 0

                    states = []
                    rewards = []
                    actions = []

class ParameterPPO:
    def __init__(self, state_dim, action_dim, action_bound, lr, gamma, clip_val, integer):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        self.gamma = gamma
        self.clip_val = clip_val
        self.action_bound = action_bound
        self.integer = str(integer)

        self.state_pl = tf.placeholder(tf.float32, [None, ] + list(state_dim), 'states_p' + self.integer)
        self.action_pl = tf.placeholder(tf.float32, shape=[None, action_dim], name="actions_p"+ self.integer)
        self.return_pl = tf.placeholder(tf.float32, [None, 1], 'return_p'+ self.integer)
        self.advantage_pl = tf.placeholder(tf.float32, [None, 1], 'advantages_p'+ self.integer)

        self.dataset = tf.data.Dataset.from_tensor_slices({"state": self.state_pl,
                                                           "actions": self.action_pl,
                                                           "return": self.return_pl,
                                                           "advantage": self.advantage_pl})
        self.dataset = self.dataset.shuffle(buffer_size=10000)
        self.dataset = self.dataset.batch(32)
        self.dataset = self.dataset.cache()
        self.dataset = self.dataset.repeat(10)
        self.iterator = self.dataset.make_initializable_iterator()
        batch = self.iterator.get_next()

        pi_probs, self.v_pred, pi_params = self._build_network('pi_p'+ self.integer, batch["state"], False)
        oldpi_probs, _, oldpi_params = self._build_network('old_pi_p'+ self.integer, batch["state"], False)
        pi_eval, self.pi_value, _ = self._build_network('pi_p'+ self.integer, self.state_pl, True)
        self.act_random = tf.squeeze(pi_eval.sample(1), axis=1)
        self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]
        # act_one_hot = tf.one_hot(self.action_pl, self.action_dim)
        # act_prob = tf.reduce_sum(act_one_hot * pi_probs, axis=1)
        # oldact_one_hot = tf.one_hot(self.action_pl, self.action_dim)
        # old_act_prob = tf.reduce_sum(oldact_one_hot * oldpi_probs, axis=1)
        act_prob = pi_probs.log_prob(batch["actions"])
        old_act_probs = oldpi_probs.log_prob(batch["actions"])
        with tf.variable_scope('critic_loss'):
            self.critic_loss = tf.reduce_mean(tf.square(batch["return"] - self.v_pred))
        with tf.variable_scope('actor_loss'):
            rt = tf.exp(act_prob - old_act_probs)
            clipped_rt = tf.clip_by_value(rt, 1 - self.clip_val, 1 + clip_val)
            self.actor_loss = tf.reduce_mean(tf.minimum(batch["advantage"] * rt,
                                                        batch["advantage"] * clipped_rt))
        # with tf.variable_scope('entropy_loss'):
        #    entropy = -tf.reduce_sum(pi_probs * tf.log(pi_probs + 1e-5), axis=1)
        #    self.entropy_loss = tf.reduce_mean(entropy, axis=0)
        with tf.variable_scope('loss_p'+ self.integer):
            self.total_loss = -self.actor_loss + self.critic_loss * 0.5  # - 0.01 * self.entropy_loss
            optimizer = tf.train.AdamOptimizer(self.lr)
            self.train_op = optimizer.minimize(self.total_loss, var_list=pi_params)

    def _build_network(self, scope, state_in, reuse):
        with tf.variable_scope(scope, reuse=reuse):
            with tf.variable_scope('policy_p'+ self.integer):
                hidden1_layer = tf.layers.dense(state_in, 400, activation=tf.nn.relu, name="p_1"+ self.integer)
                hidden2_layer = tf.layers.dense(hidden1_layer, 400, activation=tf.nn.relu, name="p_2"+ self.integer)
                mu = tf.layers.dense(hidden2_layer, self.action_dim, activation=tf.nn.tanh, name="mu"+ self.integer)
                # probs = tf.nn.softmax(output)
                log_sigma = tf.get_variable("pi_sigma_p"+ self.integer, shape=self.action_dim, initializer=tf.zeros_initializer())
                dist = tf.distributions.Normal(loc=mu * self.action_bound, scale=tf.maximum(tf.exp(log_sigma), 0.0))

            with tf.variable_scope('critic_p'):
                hidden1_layer = tf.layers.dense(state_in, 400, activation=tf.nn.relu, name="pc_1"+ self.integer)
                hidden2_layer = tf.layers.dense(hidden1_layer, 400, activation=tf.nn.relu, name="pc_2"+ self.integer)
                v_pred = tf.layers.dense(hidden2_layer, 1, name="v_pred_p"+ self.integer)
        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)
        return dist, v_pred, params

    def get_action(self, sess, state):
        return sess.run(self.act_random, feed_dict={self.state_pl: state})

    def get_value(self, sess, state):
        return sess.run(self.pi_value, feed_dict={self.state_pl: state})

    def update_params(self, sess):

        # Noisy parameters here
        old_params = []
        for param in self.pi_params:
            old_params.append(param)
            noisy_param = param + np.random.normal(0, self.noise_std, size=param.shape)
            param.assign(noisy_param)
        sess.run(self.update_oldpi_op)
        i = 0
        for param in self.pi_params:
            param.assign(old_params[i])
            i += 1

    def train(self, sess, states, actions, returns, advantages):
        feed_dict = {self.state_pl: states, self.return_pl: returns, self.advantage_pl: advantages,
                     self.action_pl: actions}
        sess.run([self.update_oldpi_op, self.iterator.initializer], feed_dict=feed_dict)
        while True:
            try:
                sess.run(self.train_op)
            except tf.errors.OutOfRangeError:
                break

class ContinuousPPO:
    def __init__(self, state_dim, action_dim, lr, gamma, clip_val, action_bound, integer):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        self.gamma = gamma
        self.clip_val = clip_val
        self.action_bound = action_bound
        self.integer = str(integer)

        self.state_pl = tf.placeholder(tf.float32, [None, ] + list(state_dim), "states" + self.integer)
        self.action_pl = tf.placeholder(tf.float32, shape=[None, action_dim], name="actions" + self.integer)
        self.advantage_pl = tf.placeholder(tf.float32, [None, 1], 'advantages' + self.integer)
        self.return_pl = tf.placeholder(tf.float32, shape=[None, 1], name="return_pl" + self.integer)
        # makes a tensor slice with all the placeholders
        self.dataset = tf.data.Dataset.from_tensor_slices({"states": self.state_pl,
                                                           "actions": self.action_pl,
                                                           "advantage": self.advantage_pl,
                                                           "return_pl": self.return_pl})

        self.dataset = self.dataset.shuffle(buffer_size=10000)
        self.dataset = self.dataset.batch(32)
        # cache to use later
        self.dataset = self.dataset.cache()
        # similar to epoch
        self.dataset = self.dataset.repeat(10)
        self.iterator = self.dataset.make_initializable_iterator()
        # one batch of 32 data
        batch = self.iterator.get_next()

        pi_probs, self.v_pred, pi_params = self._build_network('pi_c' + self.integer, batch["states"], False)
        oldpi_probs, _, oldpi_params = self._build_network('old_pi_c' + self.integer, batch["states"], False)
        pi_eval, self.pi_value, _ = self._build_network("pi_c" + self.integer, self.state_pl, True)

        self.act_random = tf.squeeze(pi_eval.sample(1), axis=1)

        # self.act_random = tf.squeeze(tf.multinomial(tf.log(pi_probs), 1))
        self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]
        act_prob = pi_probs.prob(batch["actions"])
        old_act_prob = oldpi_probs.prob(batch["actions"])

        with tf.variable_scope('critic_loss_c'+ self.integer):
            self.critic_loss = tf.reduce_mean(tf.square(batch["return_pl"] - self.v_pred))
            # (returns_pl - V(S))^2 reward +gamma*V(S')
        with tf.variable_scope('actor_loss_c'+ self.integer):
            rt = tf.exp(tf.log(act_prob) - tf.log(old_act_prob))
            clipped_rt = tf.clip_by_value(rt, 1 - self.clip_val, 1 + clip_val)
            self.actor_loss = tf.reduce_mean(tf.minimum(batch["advantage"] * rt,
                                                        batch["advantage"] * clipped_rt))
        with tf.variable_scope('loss_c'+ self.integer):
            self.total_loss = -self.actor_loss + self.critic_loss * 0.5  # - 0.01 * self.entropy_loss
            optimizer = tf.train.AdamOptimizer(self.lr)
            self.train_op = optimizer.minimize(self.total_loss, var_list=pi_params)

    def _build_network(self, scope, state, reuse):
        with tf.variable_scope(scope, reuse=reuse):
            with tf.variable_scope('policy_c'+ self.integer):
                hidden1_layer = tf.layers.dense(state, 400, activation=tf.nn.elu, name="c_1"+ self.integer)
                hidden2_layer = tf.layers.dense(hidden1_layer, 400, activation=tf.nn.elu, name="c_2"+ self.integer)
                mu = tf.layers.dense(hidden2_layer, self.action_dim, activation=tf.nn.tanh, name="mu_c"+ self.integer)
                log_sigma = tf.get_variable(name="pi_sigma_c"+ self.integer, shape=self.action_dim, initializer=tf.zeros_initializer())

                # Continuous
                dist = tf.distributions.Normal(loc=mu * self.action_bound, scale=tf.maximum(tf.exp(log_sigma), 0.0))

            with tf.variable_scope('critic'):
                hidden1_layer = tf.layers.dense(state, 400, activation=tf.nn.elu, name="cc_1"+ self.integer)
                hidden2_layer = tf.layers.dense(hidden1_layer, 400, activation=tf.nn.elu, name="cc_2"+ self.integer)
                v_pred = tf.layers.dense(hidden2_layer, 1, name="vpred_c"+ self.integer)

        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)
        return dist, v_pred, params

    def get_action(self, sess, state):
        return sess.run(self.act_random, feed_dict={self.state_pl: state})

    def get_value(self, sess, state):
        return sess.run(self.pi_value, feed_dict={self.state_pl: state})

    def update_params(self, sess):
        sess.run(self.update_oldpi_op)

    def train(self, sess, states, actions, returns, advantages):
        feed_dict = {self.state_pl: states, self.return_pl: returns, self.advantage_pl: advantages,
                     self.action_pl: actions}
        #
        sess.run([self.update_oldpi_op, self.iterator.initializer], feed_dict=feed_dict)
        while True:
            try:
                sess.run(self.train_op)
            except tf.errors.OutOfRangeError:
                break

def run1():
    env = gym.make('Pendulum-v0')
    env.seed(0)
    gamma = 0.99

    # everything we need for training
    states = []
    actions = []
    rewards = []
    values = []
    # keep track of ends
    terminals = []
    train_step, done = 0, False
    data = []

    with tf.Session() as sess:
        states_size = env.observation_space.shape
        actions_size = env.action_space.shape[0]
        action_bound = (env.action_space.high - env.action_space.low)/2
        agent = ContinuousPPO(states_size, actions_size, 0.01, gamma, 0.2, action_bound, u)
        tf.global_variables_initializer().run()
        train_step = 0
        for iteration in range(10000):
            total_reward = 0
            obs = env.reset()
            while True:
                # if iteration % 100 == 0:
                #     env.render()
                action = agent.get_action(sess, obs[np.newaxis,])
                value = agent.get_value(sess, obs[np.newaxis,])[0][0]
                new_obs, reward, new_done, info = env.step(action[0])

                if train_step == 2000:
                    rewards = np.array(rewards)
                    rewards = np.clip(rewards / (np.std(rewards) + 1e-6), -10, 10)
                    valuez = np.array(values + [value * (1 - done)])
                    terminals = np.array(terminals + [done])
                    delta = rewards + gamma * valuez[1:] * (1 - terminals[1:]) - valuez[:-1]
                    advantages = discount(delta, gamma * 0.95, terminals)
                    returns = advantages + np.array(values)
                    advantages -= np.mean(advantages)
                    advantages /= np.std(advantages) + 1e-6
                    # change to 2D
                    actions = np.vstack(actions)
                    returns = np.vstack(returns)
                    advantages = np.vstack(advantages)
                    states = np.array(states)
                    states = np.reshape(states, (train_step, ) +  agent.state_dim)
                    agent.train(sess, states, actions, returns, advantages)

                    states = []
                    actions = []
                    rewards = []
                    values = []
                    # keep track of ends
                    terminals = []
                    train_step = 0

                total_reward += reward
                states.append(obs)
                actions.append(action)
                rewards.append(reward)
                terminals.append(done)
                values.append(value)

                done = new_done
                obs = new_obs

                train_step += 1

                if done:
                    v_preds_next = values[1:] + [0]
                    #print(total_reward)
                    #print(iteration)
                    print("Continuous: ", total_reward, iteration)
                    data.append((iteration, total_reward))
                    break
        return data

def run2():
    env = gym.make('Pendulum-v0')
    # env._max_episode_steps = 9000
    env.seed(0)
    gamma = 0.99
    states = []
    actions = []
    rewards = []
    values = []
    terminals = []
    train_step, done = 0, False;
    data = []

    with tf.Session() as sess:
        state_size = env.observation_space.shape
        action_size = env.action_space.shape[0]
        action_bound = (env.action_space.high - env.action_space.low) / 2
        agent = ParameterPPO(state_size, action_size, action_bound, 1e-4, gamma, 0.2, u)
        tf.global_variables_initializer().run()
        for iteration in range(10000):
            total_reward = 0
            obs = env.reset()
            while True:
                # if iteration % 100 == 0:
                #   env.render()
                action = agent.get_action(sess, obs[np.newaxis, :])
                value = agent.get_value(sess, obs[np.newaxis, :])[0][0]
                new_obs, reward, new_done, info = env.step(action[0])
                if train_step == 2000:
                    rewards = np.array(rewards)
                    rewards = np.clip(rewards / (np.std(rewards) + 1e-6), -10, 10)
                    valuez = np.array(values + [value * (1 - done)])
                    terminals = np.array(terminals + [done])
                    delta = rewards + gamma * valuez[1:] * (1 - terminals[1:]) - valuez[: -1]
                    advantages = discount(delta, gamma * 0.95, terminals)
                    returns = advantages + np.array(values)
                    advantages -= np.mean(advantages)
                    advantages /= np.std(advantages) + 1e-6
                    actions = np.vstack(actions)
                    returns = np.vstack(returns)
                    advantages = np.vstack(advantages)
                    states = np.array(states)
                    states = np.reshape(states, (train_step,) + agent.state_dim)
                    agent.train(sess, states, actions, returns, advantages)

                    states = []
                    actions = []
                    rewards = []
                    values = []
                    terminals = []
                    train_step = 0

                total_reward += reward
                states.append(obs)
                actions.append(action)
                rewards.append(reward)
                terminals.append(done)
                values.append(value)
                done = new_done
                obs = new_obs
                train_step += 1;
                if done:
                    # values_next = values[1:] + [0]
                    print(total_reward, iteration)
                    data.append((iteration, total_reward))
                    break
        return data


def get_gaes(gamma, rewards, v_preds, v_preds_next):
    deltas = [r_t + gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]
    gaes = copy.deepcopy(deltas)
    for t in reversed(range(len(gaes) - 1)):
        gaes[t] = gaes[t] + gamma * gaes[t + 1]
    return gaes

def preprocess(input_observation, prev_processed_observation):
   obs = cv2.cvtColor(input_observation, cv2.COLOR_RGB2GRAY)
   obs[obs != 0] = 1
   obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)

   # subtract the previous frame from the current one so we are only processing on changes in the game
   if prev_processed_observation is not None:
       return np.maximum(obs[:, :, None], 0.6 * prev_processed_observation)
   return obs[:, :, None]

def discount(x, gamma, terminal_array=None):
   if terminal_array is None:
       return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]
   else:
       y, adv = 0, []
       terminals_reversed = terminal_array[1:][::-1]
       for step, dt in enumerate(reversed(x)):
           y = dt + gamma * y * (1 - terminals_reversed[step])
           adv.append(y)
       return np.array(adv)[::-1]

def plot_xy(array):
    x = []
    y = []
    for i in array:
        x.append(i[0])
        y.append(i[1])

    return x, y


continuousPPO_data = []
parameterPPO_data = []

range_data = 6

for i in range(range_data):
    continuousPPO_data.append(run1())
    parameterPPO_data.append(run2())
    u += 1

upgraded_np = np.array(continuousPPO_data)

for i in range(range_data):
    x, y1 = plot_xy(continuousPPO_data[i])
    x, y2 = plot_xy(parameterPPO_data[i])

    plt.subplot(2, 1, 1)
    plt.plot(x, y1, linewidth=3.0)
    plt.title("Normal Continuous PPO and Parameter Noise Comparison", fontsize=20)
    plt.ylabel("Continuous PPO", fontsize=20)
    plt.subplot(2, 1, 2)
    plt.plot(x, y2, linewidth=3.0)
    plt.ylabel("Parameter Noise PPO", fontsize=20)
    plt.xlabel("Episode", fontsize=20)

plt.show()
